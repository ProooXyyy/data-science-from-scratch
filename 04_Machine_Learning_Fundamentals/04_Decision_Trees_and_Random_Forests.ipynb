{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.4: Decision Trees and Random Forests\n",
    "\n",
    "Linear and Logistic Regression are powerful, but they work best when the relationship between features and the target is linear. What happens when the patterns are more complex? We turn to non-linear models like **Decision Trees**. ðŸŒ³\n",
    "\n",
    "**What is a Decision Tree?**\n",
    "A Decision Tree is like a flowchart of `if-then` questions. It splits the data into smaller and smaller branches based on its features to make a final prediction. They are highly intuitive and easy to interpret.\n",
    "\n",
    "\n",
    "\n",
    "**The Problem with a Single Tree:** A single Decision Tree can easily **overfit** the training data. It might learn the data's noise and quirks too well, causing it to perform poorly on new, unseen data.\n",
    "\n",
    "**The Solution: Random Forests!**\n",
    "A Random Forest is an **ensemble** model. Instead of one tree, it builds an entire forest of different Decision Trees and takes a majority vote for the final prediction. This \"wisdom of the crowd\" approach makes it much more robust and accurate.\n",
    "\n",
    "**Goal of this Notebook:**\n",
    "1.  Train a single Decision Tree Classifier.\n",
    "2.  Train a Random Forest Classifier on the same data.\n",
    "3.  Compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup\n",
    "\n",
    "We will reuse the `Social_Network_Ads.csv` dataset from the previous notebook. This allows us to directly compare the performance of these new models against Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../02_Data_Analysis_and_Wrangling/data/Social_Network_Ads.csv')\n",
    "\n",
    "# Define Features and Target\n",
    "X = df[['Age', 'EstimatedSalary']]\n",
    "y = df['Purchased']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note:** Decision Trees and Random Forests do **not** require feature scaling. They are not sensitive to the variance in the data, which is a major advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "dt_model = DecisionTreeClassifier(random_state=0)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"--- Decision Tree Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, dt_predictions):.2f}\\n\")\n",
    "print(classification_report(y_test, dt_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "# n_estimators is the number of trees in the forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"--- Random Forest Results ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, rf_predictions):.2f}\\n\")\n",
    "print(classification_report(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison\n",
    "\n",
    "Let's compare the results:\n",
    "* **Logistic Regression Accuracy:** ~88%\n",
    "* **Decision Tree Accuracy:** ~92%\n",
    "* **Random Forest Accuracy:** ~92%\n",
    "\n",
    "In this case, both the Decision Tree and the Random Forest outperform the simpler Logistic Regression model. The Random Forest provides the same high accuracy as the single tree but is generally more reliable and less prone to overfitting on new data, making it the preferred choice in most situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… What's Next?\n",
    "\n",
    "You've now trained and compared three of the most common classification algorithms!\n",
    "\n",
    "In the final notebook of this module, we will formalize our understanding of **Model Evaluation Metrics**. We'll review the metrics we've used and introduce a powerful visualization tool called the ROC curve to better understand the trade-offs in a model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}