{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5.1: Natural Language Processing (NLP) with NLTK\n",
    "\n",
    "Welcome to the advanced topics module! We're starting with **Natural Language Processing (NLP)**, a subfield of Artificial Intelligence focused on enabling computers to understand, interpret, and generate human language. 🗣️↔️💻\n",
    "\n",
    "From chatbots and language translation to sentiment analysis and spam detection, NLP is the driving force behind many of the AI applications we use daily.\n",
    "\n",
    "**Goal of this Notebook:**\n",
    "This is not just a coding tutorial. We will dive into the **fundamental theory** of how we prepare text data for machine learning. This process is often called the **NLP pipeline**. We will cover:\n",
    "\n",
    "1.  **The Core Challenge:** Why is human language so difficult for computers?\n",
    "2.  **The NLP Pre-processing Pipeline:** A theoretical overview.\n",
    "3.  **Tokenization:** Breaking down text into meaningful units.\n",
    "4.  **Stop Word Removal:** Filtering out common, non-informative words.\n",
    "5.  **Stemming & Lemmatization:** Normalizing words to their root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Challenge: Ambiguity and Context\n",
    "\n",
    "Human language is inherently ambiguous. Consider the sentence: \"*I saw a man on a hill with a telescope.*\"\n",
    "\n",
    "Does this mean:\n",
    "* You saw a man who was holding a telescope?\n",
    "* You were on a hill and saw a man using a telescope?\n",
    "* You were using a telescope to see a man on a hill?\n",
    "\n",
    "Humans use context to understand the intended meaning. Computers, however, see only a sequence of characters. The goal of NLP pre-processing is to break down this complex, unstructured text into a structured, numerical format that a machine learning model can understand, while trying to preserve as much of the core meaning as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The NLP Pre-processing Pipeline\n",
    "\n",
    "Before we can analyze text, we must clean and standardize it. A typical pipeline involves several steps:\n",
    "\n",
    "1.  **Raw Text:** The original, unstructured text.\n",
    "2.  **Tokenization:** Split text into individual words (tokens).\n",
    "3.  **Normalization:** Convert all tokens to lowercase.\n",
    "4.  **Stop Word Removal:** Remove common words like 'the', 'is', 'a'.\n",
    "5.  **Stemming/Lemmatization:** Reduce words to their root form (e.g., 'running' -> 'run').\n",
    "6.  **Vectorization:** Convert the clean tokens into numerical vectors (this is a more advanced topic we won't cover here, but it's the final step before machine learning).\n",
    "\n",
    "We will now implement steps 2-5 using **NLTK (Natural Language Toolkit)**, a foundational library for NLP in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing NLTK's Data Models\n",
    "\n",
    "NLTK is a powerful library, but it keeps its code separate from the data models it uses (like dictionaries, lists of words, and pre-trained algorithms). We need to download these data packages separately.\n",
    "\n",
    "* **`punkt`**: A pre-trained model that knows how to split text into sentences and then words (tokenization).\n",
    "* **`stopwords`**: A pre-compiled list of common, low-information words for various languages (e.g., 'a', 'the', 'in').\n",
    "* **`wordnet`**: A large lexical database of English words that helps NLTK understand relationships between words, which is essential for accurate lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Troubleshooting Note:** If you run the code below and get a `LookupError`, it means NLTK can't find the data you just downloaded. The best solution is to restart the kernel (**Kernel > Restart Kernel**) and run the cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "\n",
    "**Theory:** Tokenization is the process of splitting a string of text into a list of smaller units, or \"tokens\". The most common form is **word tokenization**, where the text is split by spaces and punctuation. This is the very first step in making text manageable for a computer.\n",
    "\n",
    "**Example:** `\"The cat sat on the mat.\"` becomes `['The', 'cat', 'sat', 'on', 'the', 'mat', '.']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"Data science is an amazing field. It involves a lot of learning and practice!\"\n",
    "\n",
    "tokens = word_tokenize(sample_text.lower()) # We also convert to lowercase here\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop Word Removal\n",
    "\n",
    "**Theory:** Stop words are common words in a language (like 'the', 'a', 'in', 'is', 'it') that occur very frequently but carry little semantic meaning. By removing them, we reduce the noise in our data and allow the model to focus on the more important words.\n",
    "\n",
    "**Example:** In `['the', 'cat', 'sat', 'on', 'the', 'mat']`, the stop words are `['the', 'on', 'the']`. After removal, we get `['cat', 'sat', 'mat']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "print(\"--- Tokens before stop word removal ---\")\n",
    "print(tokens)\n",
    "print(\"\\n--- Tokens after stop word removal ---\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming & Lemmatization\n",
    "\n",
    "**Theory:** These are two techniques for **normalization**—the process of reducing a word to its base or root form. This is crucial because a model should understand that 'run', 'runs', and 'running' all refer to the same concept.\n",
    "\n",
    "**Stemming:** A crude, rule-based process that chops off the ends of words. It's fast but can sometimes be inaccurate.\n",
    "* `studies`, `studying` -> `studi`\n",
    "* `ponies` -> `poni`\n",
    "\n",
    "**Lemmatization:** A more sophisticated process that uses a dictionary (like WordNet) to find the actual root form of a word, known as the **lemma**.\n",
    "* `studies`, `studying` -> `study`\n",
    "* `ponies` -> `pony`\n",
    "* `better` -> `good`\n",
    "\n",
    "Generally, **lemmatization is preferred** because it produces a valid word, though it is computationally slower than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"--- Original Filtered Tokens ---\")\n",
    "print(filtered_tokens)\n",
    "print(\"\\n--- Stemmed Tokens ---\")\n",
    "print(stemmed_tokens)\n",
    "print(\"\\n--- Lemmatized Tokens ---\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ What's Next?\n",
    "\n",
    "You now understand the foundational pre-processing steps that are required for almost any NLP task. This process of cleaning and normalizing text is what turns messy human language into structured data ready for machine learning.\n",
    "\n",
    "In the next notebook, we'll explore another advanced topic: **Time Series Forecasting**, where the goal is to predict future values based on historical data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}